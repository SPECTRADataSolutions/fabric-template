version: "3.0.0"
stage: "source"
source: "zephyr"
purpose: "SPECTRA-grade source stage with SDK-driven architecture, comprehensive testing, and dashboard-ready portfolio."
owner: "Data Platform"

inputs:
  auth_method: "apiToken"
  variable_library: "zephyrVariables"
  variables:
    # Clean, non-prefixed variable names in Variable Library
    - "base_url"          # Zephyr API base URL
    - "api_token"         # Zephyr API authentication token
    - "base_path"         # Zephyr API base path
    - "source_system"     # Source system identifier (zephyr)
    - "contract_version"  # Contract version (3.0.0)
  
  sdk:
    name: "spectraSDK"
    version: "0.3.0"
    maturity: "L1 - MVP"
    location: "spectraSDK.Notebook"
    
  exclusions:
    - "No secrets in contract; reference variable names only."
    - "Secrets stored in Fabric Variable Library and loaded at runtime."

obligations:
  core:
    - "Auth succeeds against Zephyr base URL + path for all accessible endpoints."
    - "Hierarchical access validated: Projects → Releases → Cycles → Executions → Test Cases."
    - "Endpoint catalog comprehensive (224 endpoints embedded in SDK)."
    - "Dynamic project discovery via API (no hardcoded project IDs)."
    - "Hierarchical access validated across all levels (Projects → Releases → Cycles → Executions → Test Cases)."
    - "Known limitations documented with root cause analysis."
    
  data_outputs:
    - "source.portfolio: Metadata table for source system dashboard (auth status, endpoint metrics, capabilities)."
    - "source.config: Runtime configuration snapshot."
    - "source.credentials: Masked credential validation status."
    - "source.endpoints: Complete endpoint catalog (224 endpoints, zero duplicates)."
    
  quality:
    - "Comprehensive test suite with >75% coverage (pytest + GitHub Actions)."
    - "CI/CD pipeline validates all changes automatically."
    - "Data validation functions for API responses, schemas, row counts, nulls, duplicates."
    - "Error handling with retry logic, graceful degradation, structured reporting (ErrorClassification, APIRequestHandler, ErrorCollector)."
    - "Activity logging with execution metadata (log.source table)."
    - "Discord notifications for stage completion and exceptions."
    - "Data quality quarantining infrastructure ready in SDK (DataQualityHelpers)."
    - "Comprehensive manifest entry recorded with quality gates."

dependencies:
  workspace_id: "16490dde-33b4-446e-8120-c12b0a68ed88"
  pipeline_id: "be9d0663-d383-4fe6-a0aa-8ed4781e9e87"
  notebook_id: "a744c881-065a-8e92-4cbb-2f6620f5ce36"
  lakehouse_id: "5cb93b81-8923-a984-4c5b-a9ec9325ae26"
  environment_id: "92a8349b-6a62-b2e9-40bf-1ac52e9ab184"
  
  sdk_dependencies:
    - "spectraSDK.Notebook (0.3.0)"
    - "NotebookSession (7-stage lifecycle orchestration, activity logging)"
    - "SourceStageHelpers (static methods for common operations, Discord notifications)"
    - "DataValidation (API response, schema, row count, null, duplicate checks)"
    - "ErrorClassification (HTTP/network error classification to SpectraError subclasses)"
    - "APIRequestHandler (retry logic with exponential backoff, jitter, error classification)"
    - "ErrorCollector (graceful degradation, critical/non-critical error tracking)"
    - "DataQualityHelpers (metadata-driven validation, quarantining for Extract/Clean stages)"
    - "DeltaTable (write, register, merge, optimize, vacuum)"
    - "SPECTRALogger (context-aware logging with smart debug mode)"
    - "VariableLibrary (type-safe Variable Library access)"
    - "Pipeline (interactive vs pipeline mode detection)"
    - "Environment (Fabric runtime context)"

outputs:
  manifest_path: "manifests/source.manifest.yaml"
  journal_path: "Core/memory/journal/2025-12-05-zephyr-source-spectra-grade.md"
  test_suite: "tests/"
  ci_cd_workflow: ".github/workflows/test.yml"
  
  delta_tables:
    # Portfolio and configuration
    - "Tables/source/portfolio"        # Source system metadata for dashboard
    - "Tables/source/config"           # Runtime configuration snapshot
    - "Tables/source/credentials"      # Masked credential validation
    
    # Endpoint catalog
    - "Tables/source/endpoints"        # 228 endpoints (embedded in SDK)
    
    # Note: Preview samples removed - Prepare stage creates comprehensive test data via prepare.001
  
  test_coverage:
    - "Unit tests: test_source_stage_helpers.py (>80% coverage)"
    - "Validation tests: test_validation.py (>90% coverage)"
    - "Error handling tests: test_error_handling.py (>80% coverage)"
    - "Integration tests: test_integration.py (end-to-end flows)"
    - "Overall target: >75% coverage"
  
  purpose: "Source stage provides SPECTRA-grade infrastructure with comprehensive testing and dashboard-ready portfolio. Prepare stage creates test data for schema discovery."

validation:
  completed_date: "2025-12-05"
  validation_approach: "spectra_grade"
  
  api_authentication:
    status: "validated"
    method: "Bearer Token"
    auth_failures: 0
    dynamic_project_discovery: true
    
  hierarchical_access:
    validated: true
    architecture: "hierarchical"
    levels: 5
    flow: "Projects → Releases → Cycles → Executions → Test Cases"
    extraction_errors: 0
    
  endpoint_catalog:
    total_endpoints: 228
    embedded_in_sdk: true
    categories: ["projects", "releases", "cycles", "executions", "testcases", "testers", "folders"]
    hierarchical_endpoints: 84
    flat_endpoints: 144
    
  # Preview samples removed - Prepare stage creates comprehensive test data via prepare.001
    
  test_suite:
    framework: "pytest"
    fixtures: "conftest.py (mock Spark, DeltaTable, Logger, NotebookSession)"
    coverage_target: ">75%"
    test_files:
      - "test_source_stage_helpers.py"
      - "test_validation.py"
      - "test_error_handling.py"
      - "test_integration.py"
    ci_cd: "GitHub Actions (.github/workflows/test.yml)"
    
  data_validation:
    api_response_validation: true
    schema_validation: true
    row_count_validation: true
    null_value_checks: true
    uniqueness_checks: true
    composite_quality_checks: true

quality_gates:
  authentication:
    - "✅ Auth succeeds against Zephyr API with Bearer Token"
    - "✅ No auth failures across all tested endpoints"
    - "✅ Credentials masked in source.credentials table (***_{last_3_chars})"
    
  hierarchical_access:
    - "✅ Hierarchical access validated across 5 levels"
    - "✅ Dynamic project discovery via API (no hardcoded IDs)"
    - "✅ Hierarchical access validated across all 5 levels"
    
  endpoint_catalog:
    - "✅ 228 endpoints catalogued and embedded in SDK"
    - "✅ Endpoint categories mapped to resource types"
    - "✅ Hierarchical endpoints identified and flagged"
    
  data_quality:
    - "✅ Endpoint catalog complete with full metadata (full_path, query_parameters, path_parameters)"
    - "✅ Data validation functions implemented (API, schema, row count, nulls, duplicates)"
    - "✅ Error handling with retry, graceful degradation, structured reporting"
    
  testing:
    - "✅ Comprehensive test suite with >75% coverage target"
    - "✅ CI/CD pipeline with GitHub Actions"
    - "✅ Automated coverage reporting to Codecov"
    
  documentation:
    - "✅ Contract and manifest updated to v3.0.0"
    - "✅ Test suite documentation (tests/README.md)"
    - "✅ Coverage badges in README.md"
    
  overall:
    status: "SPECTRA-grade"
    maturity: "L1 - MVP"
    ready_for_next_stage: true

# ═══════════════════════════════════════════════════════════════════════════
# DOWNSTREAM CONTRACT (Prepare Stage Handoff)
# ═══════════════════════════════════════════════════════════════════════════

guarantees:
  """Explicit guarantees provided to Prepare stage - stable contract for automation."""
  
  output_tables:
    """Source stage guarantees all output tables exist with documented schemas."""
    
    source.portfolio:
      """Single-row metadata table for source system dashboard."""
      guarantees:
        - "Exactly one row per source_system (idempotent overwrite)"
        - "All fields non-null except last_auth_check (nullable timestamp)"
        - "source_system, contract_version, total_endpoints always present"
        - "auth_status one of: Success, Failed, Unknown"
        - "status always 'active', is_enabled always true on successful run"
      schema:
        - "source_system: string (required, non-null)"
        - "contract_version: string (required, non-null, format: 'X.Y.Z')"
        - "discovery_date: date (required, non-null)"
        - "total_endpoints: integer (required, non-null, >= 0)"
        - "endpoint_categories: string (required, non-null, JSON object)"
        - "hierarchical_endpoints: integer (required, non-null, >= 0)"
        - "auth_method: string (required, non-null)"
        - "auth_status: string (required, non-null)"
        - "last_auth_check: timestamp (nullable, null if auth failed)"
        - "hierarchical_access_validated: boolean (required, non-null)"
        - "endpoint_success_rate: double (required, non-null, 0.0-1.0)"
        - "supports_incremental: boolean (required, non-null)"
        - "status: string (required, non-null, always 'active')"
        - "is_enabled: boolean (required, non-null, always true)"
        - "last_updated: timestamp (required, non-null)"
    
    source.config:
      """Key-value configuration snapshot of runtime execution context."""
      guarantees:
        - "Exactly 6 rows (execution_mode, operation_type, notebook_name, stage, sdk_version, bootstrap_enabled)"
        - "All fields non-null"
        - "config_key unique, never changes"
        - "config_value reflects current execution context"
      schema:
        - "config_key: string (required, non-null, one of: execution_mode, operation_type, notebook_name, stage, sdk_version, bootstrap_enabled)"
        - "config_value: string (required, non-null)"
        - "last_updated: timestamp (required, non-null)"
    
    source.credentials:
      """Masked credential validation status."""
      guarantees:
        - "Exactly one row (api_token credential)"
        - "credential_value always masked (***{last_3_chars})"
        - "validation_status one of: Success, Failed"
        - "last_validated updated on every run"
      schema:
        - "credential_type: string (required, non-null, always 'api_token')"
        - "credential_value: string (required, non-null, masked format: '***XXX')"
        - "last_validated: timestamp (required, non-null)"
        - "validation_status: string (required, non-null, Success|Failed)"
    
    source.endpoints:
      """Complete endpoint catalog with full metadata."""
      guarantees:
        - "Exactly 224 rows (all endpoints embedded in SDK)"
        - "Zero duplicates (full_path + http_method unique)"
        - "All fields non-null except query_parameters, path_parameters (may be empty arrays)"
        - "category always present, hierarchical boolean always present"
      schema:
        - "endpoint_path: string (required, non-null)"
        - "full_path: string (required, non-null, unique per method)"
        - "http_method: string (required, non-null, GET|POST|PUT|DELETE)"
        - "category: string (required, non-null)"
        - "description: string (required, non-null)"
        - "requires_auth: boolean (required, non-null)"
        - "hierarchical: boolean (required, non-null)"
        - "query_parameters: array<string> (required, may be empty)"
        - "path_parameters: array<string> (required, may be empty)"
        - "resource: string (required, non-null)"
  
  validation_status:
    """All tables validated before completion."""
    guarantees:
      - "All output tables exist and are registered in Spark metastore"
      - "All tables pass validation checks (row counts, schemas, required fields)"
      - "Validation errors collected and reported (non-critical errors logged, critical errors fail stage)"
      - "Activity log records validation results"
  
  completeness:
    """Source stage is complete when activity log entry exists with status='Success'."""
    guarantees:
      - "Activity log entry written to log.source table on every run"
      - "Status 'Success' only if all critical operations completed"
      - "Status 'Failed' if any critical error occurred (auth failure, table creation failure)"

metadata_envelope:
  """Metadata provided to Prepare stage for traceability and automation."""
  
  activity_log:
    """Execution metadata stored in log.source Delta table."""
    table: "log.source"
    path: "Tables/log/sourcelog"
    schema:
      - "execution_id: string (unique per execution, format: {notebook_name}_{timestamp}_{microseconds})"
      - "notebook_name: string (e.g., 'sourceZephyr')"
      - "stage: string (always 'source')"
      - "source_system: string (e.g., 'zephyr')"
      - "source_name: string (e.g., 'Zephyr Enterprise')"
      - "workspace_id: string (Fabric workspace UUID)"
      - "lakehouse_id: string (Fabric lakehouse UUID)"
      - "lakehouse_name: string (Fabric lakehouse name)"
      - "execution_mode: string ('interactive' or 'pipeline')"
      - "status: string ('Success' or 'Failed')"
      - "error_message: string (nullable, error message if failed)"
      - "capabilities: string (nullable, comma-separated capability list)"
      - "duration_seconds: double (execution duration)"
      - "error_summary: string (nullable, JSON string with error counts by category)"
      - "parameters: string (nullable, JSON string with execution parameters)"
      - "start_time: timestamp (execution start)"
      - "end_time: timestamp (execution end)"
      - "recorded_at: timestamp (log entry timestamp)"
  
  execution_id:
    """Unique identifier for each Source stage execution."""
    format: "{notebook_name}_{YYYYMMDD}_{HHMMSS}_{microseconds}"
    example: "sourceZephyr_20251208_163015_123456"
    purpose: "Correlation ID for tracing Source → Prepare → Extract stages"
  
  contract_version:
    """Version tag for contract compatibility."""
    location: "source.portfolio.contract_version"
    format: "semver (X.Y.Z)"
    current: "3.0.0"
    purpose: "Prepare stage can verify contract compatibility"
  
  timestamps:
    """Temporal metadata for batch identification."""
    execution_start: "log.source.start_time"
    execution_end: "log.source.end_time"
    last_updated: "source.portfolio.last_updated"
    purpose: "Prepare stage can determine data freshness and batch boundaries"

state_handoff:
  """Protocol for Prepare stage to determine Source completion status."""
  
  completion_marker:
    """How Prepare knows Source stage is complete."""
    method: "Activity log query"
    query: |
      SELECT * FROM log.source 
      WHERE stage = 'source' 
        AND source_system = 'zephyr'
        AND status = 'Success'
      ORDER BY recorded_at DESC 
      LIMIT 1
    guarantees:
      - "If query returns row with status='Success', Source completed successfully"
      - "If query returns row with status='Failed', Source failed (do not proceed)"
      - "If query returns no rows, Source not yet executed"
  
  readiness_check:
    """Prepare stage must verify Source readiness before proceeding."""
    checks:
      - "Activity log entry exists with status='Success' for latest execution"
      - "All output tables exist: source.portfolio, source.config, source.credentials, source.endpoints"
      - "source.portfolio.auth_status = 'Success' (authentication successful)"
      - "source.endpoints row count >= 224 (endpoint catalog complete)"
      - "source.portfolio.contract_version matches expected version"
    failure_response:
      - "If any check fails, Prepare stage should not proceed"
      - "Log readiness check failure to Prepare activity log"
      - "Raise exception or return error status"
  
  batch_identification:
    """How Prepare identifies which Source execution to consume."""
    method: "Latest successful execution"
    logic: |
      1. Query log.source for latest execution with status='Success'
      2. Use execution_id as batch identifier
      3. Use start_time/end_time as batch time window
      4. Pass execution_id to downstream stages for correlation
    guarantees:
      - "Each Source execution produces unique execution_id"
      - "Activity log preserves execution history (append-only)"
      - "Prepare always consumes latest successful execution"

versioning:
  change_log:
    - version: "3.0.0"
      date: "2025-12-05"
      maturity: "L1 - MVP"
      changes:
        - "SPECTRA-grade transformation with spectraSDK (v0.3.0)"
        - "7-stage notebook lifecycle (NotebookSession orchestration)"
        - "Renamed fabricSDK → spectraSDK for consistency"
        - "Variable Library with clean names (base_url, api_token, base_path)"
        - "New table structure: source.portfolio (dashboard-ready), source.config, source.credentials"
        - "228 endpoints embedded in SDK (no JSON file dependency)"
        - "Dynamic project discovery (no hardcoded IDs)"
        - "Endpoint catalog with full metadata (224 endpoints, zero duplicates)"
        - "Comprehensive test suite (pytest): >75% coverage target"
        - "Unit tests (test_source_stage_helpers.py): >80% coverage"
        - "Validation tests (test_validation.py): >90% coverage"
        - "Error handling tests (test_error_handling.py): >80% coverage"
        - "Integration tests (test_integration.py): end-to-end flows"
        - "CI/CD pipeline with GitHub Actions (.github/workflows/test.yml)"
        - "Automated coverage reporting to Codecov"
        - "DataValidation class: API response, schema, row count, null, duplicate checks"
        - "SourceStageHelpers class: static methods for common operations"
        - "Error handling: retry logic, graceful degradation, structured reporting"
        - "API token masking in credentials table (***_{last_3_chars})"
        - "Updated lakehouse ID to 5cb93b81-8923-a984-4c5b-a9ec9325ae26"
        - "Schema support enabled (direct schema.table notation)"
      breaking_changes:
        - "Removed DXC_ZEPHYR_* prefixed variables → clean Variable Library names"
        - "Removed source.source → source.portfolio (redesigned as dashboard metadata)"
        - "Removed legacy tables (hierarchical_validation, endpoint_health, quality_gate_report)"
        - "Removed JSON file dependency → endpoints embedded in SDK"
        - "Notebook now requires %run spectraSDK at top"
        - "Variable Library must be populated with: base_url, api_token, base_path, source_system, contract_version"
      sdk_features:
        - "NotebookSession: 7-stage lifecycle (Parameters, Context, Initialize, Execute, Validate, Record, Finalise)"
        - "SourceStageHelpers: create_source_portfolio_table, create_source_config_table, create_source_credentials_table, validate_api_authentication, validate_api_resource_access, bootstrap_endpoints_catalog"
        - "DataValidation: validate_api_response, validate_dataframe_schema, validate_row_count, check_for_nulls, check_for_duplicates, composite_data_quality_check"
        - "DeltaTable: write, register, merge, optimize, vacuum (with schema evolution)"
        - "SPECTRALogger: context-aware logging with smart debug mode"
        - "VariableLibrary: type-safe Variable Library access"
        - "Pipeline: interactive vs pipeline mode detection"
        - "Environment: Fabric runtime context (workspace, lakehouse, tenant IDs)"
      
    - version: "2.0.0"
      date: "2025-12-02"
      changes:
        - "Comprehensive endpoint testing (120 endpoints)"
        - "Discovered hierarchical API architecture"
        - "Validated all 4 hierarchy levels with actual data"
        - "Extracted sample dimensional dataset (265 rows)"
        - "Found 29 ID types including defectId and requirementId"
        - "Tested with actual IDs to prove service issues"
        - "Documented 3 confirmed limitations with workarounds"
        - "Achieved 70% endpoint success rate (84/120)"
        - "Generated 27 comprehensive deliverables"
        - "Quality score: 95/100 (Excellent)"
      breaking_changes:
        - "Updated from simple /project handshake to comprehensive validation"
        - "Added hierarchical access requirements"
        - "Changed success criteria from single endpoint to full hierarchy"
        
    - version: "1.0.0"
      date: "2025-01-29"
      changes:
        - "Initial Source contract"
        - "Basic /project endpoint handshake"
